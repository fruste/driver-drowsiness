{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6abcbf6-8bef-4423-beb9-960a57dc5d56",
   "metadata": {},
   "source": [
    "# Temporal model Deep GRU + Shallow GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57916f02-9379-471c-b5ee-3c0f6ad50e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a5e8b6b-b209-4e37-8fe4-6f70d479ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe20f1-0864-435b-ae54-3a82af6f6c35",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1960e29b-ff82-4c2f-b56c-329340f82145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(subject):\n",
    "    path = '../embeddings/embeddings_' + subject\n",
    "    normal_embs = np.load(path + '_normal.npy')\n",
    "    sleepy_embs = np.load(path + '_sleepy.npy')\n",
    "\n",
    "    return normal_embs, sleepy_embs\n",
    "\n",
    "def load_multi_embeddings(subjects):\n",
    "    normal_dict = {}\n",
    "    sleepy_dict = {}\n",
    "\n",
    "    for sub in subjects:\n",
    "        path = '../embeddings/embeddings_sub' + str(sub)\n",
    "        normal_frames = np.load(path + '_normal.npy')\n",
    "        sleepy_frames = np.load(path + '_sleepy.npy')\n",
    "\n",
    "        normal_dict[str(sub)] = normal_frames\n",
    "        sleepy_dict[str(sub)] = sleepy_frames\n",
    "\n",
    "    return normal_dict, sleepy_dict\n",
    "\n",
    "from helping_functions import *\n",
    "\n",
    "\n",
    "def get_status_rates(subject, treshhold, segment_length):\n",
    "    status_rates_sleepy, wrong_frames_sleepy = load_blinks(subject, 'sleepy') \n",
    "    status_rates_normal, wrong_frames_normal = load_blinks(subject, 'normal') \n",
    "    \n",
    "    print(\"Starting segmenting normal condition\")\n",
    "    blink_counts_normal, average_durs_normal = run_analysis(status_rates_normal, wrong_frames_normal, treshhold, segment_length)\n",
    "    print(\"Starting segmenting sleepy condition\")\n",
    "    blink_counts_sleepy, average_durs_sleepy = run_analysis(status_rates_sleepy, wrong_frames_sleepy, treshhold, segment_length)\n",
    "    \n",
    "    return list(zip(blink_counts_normal, average_durs_normal)), list(zip(blink_counts_sleepy, average_durs_sleepy))\n",
    "\n",
    "def split_into_segments(frames, gru_segment_length):\n",
    "    num_segments = len(frames) // gru_segment_length\n",
    "    frames = frames[:num_segments * gru_segment_length]\n",
    "    return np.array(np.split(np.array(frames), num_segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd569b8d-a68b-4050-b255-ae890bcc3e6f",
   "metadata": {},
   "source": [
    "## First model: GRU deep features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628ea07-9570-41ec-85c7-2df0cd68942b",
   "metadata": {},
   "source": [
    "### Model initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630324e1-d991-4099-8a21-e7e74a06b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78e999b-ba8c-49d6-bcb2-c1f083806e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Input, concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "class DrowsinessDetector:\n",
    "    def __init__(self, segment_length, gru_segment_length, attention_units=32, rnn_units=64):\n",
    "        self.segment_length = segment_length\n",
    "        self.deep_input_shape = (self.segment_length, 2048)\n",
    "        self.shallow_input_shape = (gru_segment_length, 2)\n",
    "        self.attention_units = attention_units\n",
    "        self.rnn_units = rnn_units\n",
    "    \n",
    "    def split_frames(self, frames):\n",
    "        num_segments = len(frames) // self.segment_length\n",
    "        frames = frames[:num_segments * self.segment_length]\n",
    "        return np.array(np.split(frames, num_segments))\n",
    "    \n",
    "    def create_labels(self, num_segments, label):\n",
    "        return np.array([label] * num_segments)\n",
    "    \n",
    "    def shuffle_data(self, X_shallow, X_deep, y):\n",
    "        indices = np.arange(len(X_shallow))\n",
    "        np.random.shuffle(indices)\n",
    "        return X_shallow[indices], X_deep[indices], y[indices]\n",
    "\n",
    "    def construct_model(self):\n",
    "        # Define the shallow input layer\n",
    "        shallow_input = Input(shape=self.shallow_input_shape)\n",
    "\n",
    "        # Define a feedforward network to compute the attention weights from the shallow features\n",
    "        attention_layer_1 = Dense(units=self.attention_units, activation='relu')(shallow_input)\n",
    "        attention_weights = Dense(units=1, activation='softmax')(attention_layer_1)\n",
    "\n",
    "        # Define the deep input layer and RNN\n",
    "        deep_input = Input(shape=self.deep_input_shape)\n",
    "        rnn_layer = SimpleRNN(units=self.rnn_units)(deep_input)\n",
    "\n",
    "        # Compute the weighted average of the RNN outputs using the attention weights\n",
    "        attention_layer_2 = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[1, 1]))([attention_weights, rnn_layer])\n",
    "\n",
    "        # Concatenate the outputs of the two networks\n",
    "        merged = Concatenate()([attention_layer_2, shallow_input])\n",
    "\n",
    "        # Add a dense layer for the final prediction\n",
    "        dense_1 = Dense(units=32, activation='relu')(merged)\n",
    "        dense_2 = Dense(units=1, activation='sigmoid')(dense_1)\n",
    "\n",
    "        # Define the model with the two input layers and the concatenated output\n",
    "        model = Model(inputs=[shallow_input, deep_input], outputs=dense_2)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_deep_data(self, normal_frames_dict, sleepy_frames_dict, subjects, leave_out_subjects):\n",
    "            # X_train, y_train = None, None\n",
    "            X_train, y_train = [], []\n",
    "            length_list = []\n",
    "            for subject in subjects:\n",
    "                if subject in leave_out_subjects:\n",
    "                    continue\n",
    "                print('train', subject)\n",
    "                normal_frames = normal_frames_dict[str(subject)]\n",
    "                normal_X = self.split_frames(normal_frames)\n",
    "                normal_y = self.create_labels(len(normal_X), 0)\n",
    "\n",
    "                sleepy_frames = sleepy_frames_dict[str(subject)]\n",
    "                sleepy_X = self.split_frames(sleepy_frames)\n",
    "                sleepy_y = self.create_labels(len(sleepy_X), 1)\n",
    "\n",
    "                X_train.append((normal_X, sleepy_X))\n",
    "                y_train.append((normal_y, sleepy_y))\n",
    "\n",
    "                length_list.append((len(normal_y), len(sleepy_y)))\n",
    "\n",
    "            X_test, y_test = [], []\n",
    "            for subject in leave_out_subjects:\n",
    "                print('test', subject)\n",
    "                normal_frames = normal_frames_dict[str(subject)]\n",
    "                normal_X = self.split_frames(normal_frames)\n",
    "                normal_y = self.create_labels(len(normal_X), 0)\n",
    "\n",
    "                sleepy_frames = sleepy_frames_dict[str(subject)]\n",
    "                sleepy_X = self.split_frames(sleepy_frames)\n",
    "                sleepy_y = self.create_labels(len(sleepy_X), 1)\n",
    "                \n",
    "                X_test.append((normal_X, sleepy_X))\n",
    "                y_test.append((normal_y, sleepy_y))\n",
    "                length_list.append((len(normal_y), len(sleepy_y)))\n",
    "\n",
    "            return X_train, X_test, y_train, y_test, length_list\n",
    "        \n",
    "    def get_shallow_data(self, subjects, leave_out_subjects, segment_length, gru_segment_length):\n",
    "            X_train, y_train = [], []\n",
    "            length_list = []\n",
    "            for sub in subjects:\n",
    "                subject = 'subject' + str(sub)\n",
    "\n",
    "                if sub in leave_out_subjects:\n",
    "                    continue\n",
    "\n",
    "                X_normal, X_sleepy = get_status_rates(subject, 10, segment_length)\n",
    "\n",
    "                X_normal_segmented = split_into_segments(X_normal, gru_segment_length)\n",
    "                X_sleepy_segmented = split_into_segments(X_sleepy, gru_segment_length)\n",
    "\n",
    "                y_normal = self.create_labels(len(X_normal_segmented), 0)\n",
    "                y_sleepy = self.create_labels(len(X_sleepy_segmented), 1)\n",
    "\n",
    "                X_train.append((X_normal_segmented, X_sleepy_segmented))\n",
    "                y_train.append((y_normal, y_sleepy))\n",
    "\n",
    "                length_list.append((len(y_normal), len(y_sleepy)))\n",
    "           \n",
    "            X_test, y_test = [], []\n",
    "            for sub in leave_out_subjects:\n",
    "                subject = 'subject' + str(sub)\n",
    "                print('test', subject)\n",
    "\n",
    "                X_normal, X_sleepy = get_status_rates(subject, 10, segment_length)\n",
    "\n",
    "                X_normal_segmented = split_into_segments(X_normal, gru_segment_length)\n",
    "                X_sleepy_segmented = split_into_segments(X_sleepy, gru_segment_length)\n",
    "\n",
    "                y_normal = self.create_labels(len(X_normal_segmented), 0)\n",
    "                y_sleepy = self.create_labels(len(X_sleepy_segmented), 1)\n",
    "\n",
    "                X_test.append((X_normal_segmented, X_sleepy_segmented))\n",
    "                y_test.append((y_normal, y_sleepy))\n",
    "\n",
    "                length_list.append((len(y_normal), len(y_sleepy)))\n",
    "\n",
    "            return X_train, X_test, y_train, y_test, length_list\n",
    "\n",
    "    def train(self, X_train_shallow, X_train_deep, y_train, num_epochs=10, batch_size=32, validation_split=0.2):\n",
    "        self.batch_size = batch_size\n",
    "        model = self.construct_model()\n",
    "        print('Model is constructed')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "        model.fit([X_train_shallow, X_train_deep], y_train, epochs=num_epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stop])\n",
    "\n",
    "        self.model = model\n",
    "        print(\"Done with training\")\n",
    "        return model\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        model = self.model\n",
    "        return model.evaluate(X_test, y_test, batch_size=self.batch_size)\n",
    "    \n",
    "    def get_f1(self, X_test_shallow, X_test_deep, y_test):\n",
    "        y_predict = model.predict([X_test_shallow, X_test_deep], batch_size = self.batch_size)\n",
    "        y_predict_int = np.round(y_predict).astype(int).flatten()\n",
    "        f1 = f1_score(y_test, y_predict_int)\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b2e40b-24a3-4b4c-a900-52fe8cccad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded but works (BUT ONLY FOR MULTIPLE TEST SUBJECTS)\n",
    "def equally_sized_lists(array1, array2):\n",
    "    list1 = array1[0]\n",
    "    list2 = array2[0]\n",
    "    list3 = array1[1]\n",
    "    list4 = array2[1]\n",
    "    if len(list1) > len(list2):\n",
    "        list1 = list1[:len(list2)]\n",
    "    elif len(list1) < len(list2):\n",
    "        list2 = list2[:len(list1)]\n",
    "        \n",
    "    if len(list3) > len(list4):\n",
    "        list3 = list3[:len(list4)]\n",
    "    elif len(list3) < len(list4):\n",
    "        list4 = list4[:len(list3)]\n",
    "    return [list1, list3], [list2, list4]\n",
    "\n",
    "def create_equal_arrays(shallow, deep):\n",
    "    first = True\n",
    "    for sub in range(np.array(shallow).shape[0]):\n",
    "        shallow_features = np.array(shallow[sub])\n",
    "        deep_features = np.array(deep[sub])\n",
    "        \n",
    "        shallow_features, deep_features = equally_sized_lists(shallow_features, deep_features)\n",
    "        if first:\n",
    "            normal_shallow = shallow_features[0]\n",
    "            sleepy_shallow = shallow_features[1]\n",
    "\n",
    "            normal_deep = deep_features[0]\n",
    "            sleepy_deep = deep_features[1]\n",
    "            first = False\n",
    "            continue\n",
    "        normal_shallow = np.concatenate((normal_shallow, shallow_features[0]))\n",
    "        sleepy_shallow = np.concatenate((sleepy_shallow, shallow_features[1]))\n",
    "            \n",
    "        normal_deep = np.concatenate((normal_deep, deep_features[0]))\n",
    "        sleepy_deep = np.concatenate((sleepy_deep, deep_features[1]))\n",
    "        \n",
    "    # combine list\n",
    "    shallow = np.concatenate((normal_shallow, sleepy_shallow))\n",
    "    deep = np.concatenate((normal_deep, sleepy_deep))\n",
    "            \n",
    "    return shallow, deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56916945-fea1-4162-a5ff-614b6683aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test subjects:  [20, 23, 11]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m leave_out_subjects \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m11\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest subjects: \u001b[39m\u001b[38;5;124m\"\u001b[39m, leave_out_subjects)\n\u001b[0;32m---> 11\u001b[0m normal_dict, sleepy_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_multi_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m X_deep_train, X_deep_test, y_deep_train, y_deep_test, deep_len_list \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mget_deep_data(normal_dict, sleepy_dict, subjects, leave_out_subjects)\n\u001b[1;32m     14\u001b[0m segment_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m46\u001b[39m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mload_multi_embeddings\u001b[0;34m(subjects)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m subjects:\n\u001b[1;32m     13\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../embeddings/embeddings_sub\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(sub)\n\u001b[0;32m---> 14\u001b[0m     normal_frames \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_normal.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     sleepy_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_sleepy.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     normal_dict[\u001b[38;5;28mstr\u001b[39m(sub)] \u001b[38;5;241m=\u001b[39m normal_frames\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/lib/npyio.py:413\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/numpy/lib/format.py:755\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    754\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    758\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    768\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detector = DrowsinessDetector(segment_length=60 * 46, gru_segment_length=6)\n",
    "\n",
    "#subjects = [1,3,4,6,7,9,14,15,16,20,23,24]\n",
    "subjects = [9,10, 11, 12, 14,15,16,20,23,24, 25]\n",
    "\n",
    "#leave_out_ratio = 0.3\n",
    "#leave_out_subjects = np.random.choice(subjects, int(len(subjects) * leave_out_ratio), replace=False)\n",
    "leave_out_subjects = [20, 23, 11]\n",
    "print(\"Test subjects: \", leave_out_subjects)\n",
    "\n",
    "normal_dict, sleepy_dict = load_multi_embeddings(subjects)\n",
    "X_deep_train, X_deep_test, y_deep_train, y_deep_test, deep_len_list = detector.get_deep_data(normal_dict, sleepy_dict, subjects, leave_out_subjects)\n",
    "\n",
    "segment_length = 10 * 46\n",
    "gru_segment_length = 6\n",
    "X_shallow_train, X_shallow_test, y_shallow_train, y_shallow_test, shallow_len_list = detector.get_shallow_data(subjects, leave_out_subjects, segment_length, gru_segment_length)\n",
    "\n",
    "X_shallow_train, X_deep_train = create_equal_arrays(X_shallow_train, X_deep_train)\n",
    "y_shallow_train, y_deep_train = create_equal_arrays(y_shallow_train, y_deep_train)\n",
    "\n",
    "X_shallow_test, X_deep_test = create_equal_arrays(X_shallow_test, X_deep_test)\n",
    "y_shallow_test, y_deep_test = create_equal_arrays(y_shallow_test, y_deep_test)\n",
    "\n",
    "X_shallow_train, X_deep_train, y_train = detector.shuffle_data(X_shallow_train, X_deep_train, y_shallow_train)\n",
    "X_shallow_test, X_deep_test, y_test = detector.shuffle_data(X_shallow_test, X_deep_test, y_shallow_test)\n",
    "\n",
    "print(\"Data is obtained\")\n",
    "model = detector.train(X_shallow_train, X_deep_train, y_train, num_epochs=100, batch_size=8)\n",
    "#detector.test(X_test, y_test)\n",
    "#y_test, y_predict = detector.compare_predictions(X_test, y_test)\n",
    "#print(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f65a6e6-24ca-4373-9ac3-18b896b24062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "f1 = detector.get_f1(X_shallow_test, X_deep_test, y_test)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
