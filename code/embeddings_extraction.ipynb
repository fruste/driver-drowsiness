{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Extraction\n",
    "This notebook contains the code for the extraction of the face embeddings using a VGGFace ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import cv2\n",
    "import dlib\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displayinh the number of GPUs available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a test image, resize it, and convert it to grayscale\n",
    "def load_image(path, show=False, gray_show=False):\n",
    "    test_image = cv2.imread(path)\n",
    "    test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    test_image = imutils.resize(test_image, width=500)\n",
    "    gray = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(test_image)\n",
    "        plt.show()\n",
    "        if gray_show:\n",
    "            plt.imshow(gray, cmap='gray')\n",
    "            plt.show()\n",
    "    return test_image, gray\n",
    "\n",
    "def detect_faces(face_detector, gray, img, scale_factor, frame_num, show_multi_faces = False):\n",
    "    detections = haar_cascade.detectMultiScale(gray, scaleFactor=scale_factor, minNeighbors=5, minSize=(150, 150), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    #Checking if there are more than 1 detected faces\n",
    "    if len(detections) > 1:\n",
    "        print('multiple faces', frame_num)\n",
    "        if detections[0][2] > detections[1][2]:\n",
    "            face = detections[1]\n",
    "            other = detections[0]\n",
    "        else:\n",
    "            face = detections[0]\n",
    "            other = detections[1]\n",
    "        if show_multi_faces:\n",
    "            print('largest and chosen face')\n",
    "            fX, fY, fW, fH = face[0],face[1], face[2], face[3]\n",
    "            plt.imshow(gray[fY:fY + fH, fX:fX + fW], cmap='gray')\n",
    "            plt.show()\n",
    "            print('not chosen face')\n",
    "            fX, fY, fW, fH = other[0], other[1], other[2], other[3]\n",
    "            plt.imshow(gray[fY:fY + fH, fX:fX + fW], cmap='gray')\n",
    "            plt.show()\n",
    "    elif len(detections) == 1:\n",
    "        face = detections[0]\n",
    "    # if no face detected\n",
    "    elif len(detections) == 0:\n",
    "        print('No face detected', frame_num)\n",
    "        return False\n",
    "    fX, fY, fW, fH = face[0], face[1], face[2], face[3]\n",
    "    face_img = Image.fromarray(img[fY:fY + fH, fX:fX + fW])\n",
    "    face_img = face_img.resize((224, 224))\n",
    "    #face_img.show()\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify subject and condition\n",
    "subject = 'P11407'\n",
    "condition = 'normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# haar Cascade path\n",
    "cascade_path = '../models/haarcascade_frontalface_default.xml'\n",
    "# create haar cascade\n",
    "haar_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "face_detector = 'haar'\n",
    "\n",
    "frame_start = 35000\n",
    "frame_end =  203000\n",
    "frames = range(frame_start, frame_end + 1)\n",
    "\n",
    "scale_factor = 1.03\n",
    "\n",
    "folder_path = '../frames/' + subject + '/' + condition + '/frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings(frames, folder_path, face_detector, model, scale_factor, show=False, show_multi_faces=False):\n",
    "    no_face_count = 0\n",
    "    embeddings = []\n",
    "    for frame in frames:\n",
    "        if frame % 1000 == 0:\n",
    "            print(frame)\n",
    "            gc.collect()\n",
    "        if frame == 100000:\n",
    "            np.save('embeddings_sub' + subject[-1] + '_' + condition + '_temp1.npy', embeddings)\n",
    "        if frame == 135000:\n",
    "            np.save('embeddings_sub' + subject[-1] + '_' + condition + '_temp2.npy', embeddings)\n",
    "        if frame == 160000:\n",
    "            np.save('embeddings_sub' + subject[-1] + '_' + condition + '_temp3.npy', embeddings)\n",
    "        path = folder_path + str(frame) + '.jpg'\n",
    "        test_img, gray = load_image(path, show=False)\n",
    "        face = detect_faces(face_detector, gray, test_img, scale_factor, frame, show_multi_faces=False)\n",
    "\n",
    "        if not face:\n",
    "            print(\"No face detected\", frame)\n",
    "            no_face_count += 1\n",
    "            continue\n",
    "\n",
    "        face_array = np.asarray(face)\n",
    "        pixels = face_array.astype('float32')\n",
    "        samples = np.expand_dims(pixels, axis=0)\n",
    "        samples = preprocess_input(samples, version=2)\n",
    "        embedding = model.predict(samples, verbose=0)\n",
    "        embeddings.append(embedding[0])\n",
    "    print(no_face_count)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings(frames, folder_path, face_detector, model, scale_factor)\n",
    "np.save('embeddings_sub' + subject[-1] + '_' + condition + '.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracts average embedding in a segment from given embeddigs\n",
    "def embeddings_segment(embeddings, video_len, segment_len):\n",
    "    # The amount of frames at the end that are not taken into account\n",
    "    rest = video_len % segment_len\n",
    "    num_frames = video_len - rest\n",
    "    avg_embeddings = []\n",
    "    acum_embeddings = np.zeros(2048)\n",
    "    # a blink is counted to a segment,when the blink starts in that segment\n",
    "    for frame in range(num_frames + 1):\n",
    "        if frame % 1000 == 0:\n",
    "            print(frame)\n",
    "        acum_embeddings = acum_embeddings + np.array(embeddings[frame])\n",
    "        # only happens at the end of a segment\n",
    "        if frame % segment_len == 0 and frame != 0:\n",
    "            avg_embeddings.append(acum_embeddings / segment_len)\n",
    "            acum_embeddings = np.zeros(2048)\n",
    "            #print('New segment: ', frame)\n",
    "    return avg_embeddings\n",
    "\n",
    "avg_embeddings = embeddings_segment(embeddings, len(embeddings), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pri_comps = pca.fit_transform(avg_embeddings)\n",
    "\n",
    "print(pri_comps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
