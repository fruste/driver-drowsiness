{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6abcbf6-8bef-4423-beb9-960a57dc5d56",
   "metadata": {},
   "source": [
    "# Temporal model Deep GRU + Shallow GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57916f02-9379-471c-b5ee-3c0f6ad50e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5e8b6b-b209-4e37-8fe4-6f70d479ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe20f1-0864-435b-ae54-3a82af6f6c35",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1960e29b-ff82-4c2f-b56c-329340f82145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(subject):\n",
    "    path = '../embeddings/embeddings_' + subject\n",
    "    normal_embs = np.load(path + '_normal.npy')\n",
    "    sleepy_embs = np.load(path + '_sleepy.npy')\n",
    "\n",
    "    return normal_embs, sleepy_embs\n",
    "\n",
    "def load_multi_embeddings(subjects):\n",
    "    normal_dict = {}\n",
    "    sleepy_dict = {}\n",
    "\n",
    "    for sub in subjects:\n",
    "        path = '../embeddings/embeddings_sub' + str(sub)\n",
    "        normal_frames = np.load(path + '_normal.npy')\n",
    "        sleepy_frames = np.load(path + '_sleepy.npy')\n",
    "\n",
    "        normal_dict[str(sub)] = normal_frames\n",
    "        sleepy_dict[str(sub)] = sleepy_frames\n",
    "\n",
    "    return normal_dict, sleepy_dict\n",
    "\n",
    "from helping_functions import *\n",
    "\n",
    "\n",
    "def get_status_rates(subject, treshhold, segment_length):\n",
    "    status_rates_sleepy, wrong_frames_sleepy = load_blinks(subject, 'sleepy') \n",
    "    status_rates_normal, wrong_frames_normal = load_blinks(subject, 'normal') \n",
    "    \n",
    "    print(\"Starting segmenting normal condition\")\n",
    "    blink_counts_normal, average_durs_normal = run_analysis(status_rates_normal, wrong_frames_normal, treshhold, segment_length)\n",
    "    print(\"Starting segmenting sleepy condition\")\n",
    "    blink_counts_sleepy, average_durs_sleepy = run_analysis(status_rates_sleepy, wrong_frames_sleepy, treshhold, segment_length)\n",
    "    \n",
    "    return list(zip(blink_counts_normal, average_durs_normal)), list(zip(blink_counts_sleepy, average_durs_sleepy))\n",
    "\n",
    "def split_into_segments(frames, gru_segment_length):\n",
    "    num_segments = len(frames) // gru_segment_length\n",
    "    frames = frames[:num_segments * gru_segment_length]\n",
    "    return np.array(np.split(np.array(frames), num_segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd569b8d-a68b-4050-b255-ae890bcc3e6f",
   "metadata": {},
   "source": [
    "## First model: GRU deep features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628ea07-9570-41ec-85c7-2df0cd68942b",
   "metadata": {},
   "source": [
    "### Model initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630324e1-d991-4099-8a21-e7e74a06b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d78e999b-ba8c-49d6-bcb2-c1f083806e71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Adam' from 'keras.optimizers' (/opt/conda/lib/python3.9/site-packages/keras/optimizers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, GRU, Input, concatenate\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Adam' from 'keras.optimizers' (/opt/conda/lib/python3.9/site-packages/keras/optimizers.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Input, concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "class DrowsinessDetector:\n",
    "    def __init__(self, segment_length, gru_segment_length):\n",
    "        self.segment_length = segment_length\n",
    "        self.deep_input_shape = (self.segment_length, 2048)\n",
    "        self.shallow_input_shape = (gru_segment_length, 2)\n",
    "        \n",
    "    def split_frames(self, frames):\n",
    "        num_segments = len(frames) // self.segment_length\n",
    "        frames = frames[:num_segments * self.segment_length]\n",
    "        return np.array(np.split(frames, num_segments))\n",
    "    \n",
    "    def create_labels(self, num_segments, label):\n",
    "        return np.array([label] * num_segments)\n",
    "    \n",
    "    def shuffle_data(self, X, y):\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def construct_model(self):\n",
    "        # Define the shallow input layer and GRU\n",
    "        shallow_input = Input(shape=self.shallow_input_shape)\n",
    "        shallow_gru = SimpleRNN(8)(shallow_input)\n",
    "\n",
    "        # Define the deep input layer and GRU\n",
    "        deep_input = Input(shape=self.deep_input_shape)\n",
    "        deep_gru = SimpleRNN(64)(deep_input)\n",
    "\n",
    "        # Concatenate the outputs of the two GRUs\n",
    "        merged = concatenate([shallow_gru, deep_gru])\n",
    "\n",
    "        # Add a dense layer to control how much focus is put on the shallow and deep features\n",
    "        dense_1 = Dense(32, activation='relu')(merged)\n",
    "\n",
    "        # Add a dense layer for the final prediction\n",
    "        dense_2 = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "        # Define the model with the two input layers and the concatenated output\n",
    "        model = Model(inputs=[shallow_input, deep_input], outputs=dense_2)\n",
    "\n",
    "        optimizer = Adam(lr=0.0001)\n",
    "        # Compile the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_deep_data(self, normal_frames_dict, sleepy_frames_dict, subjects, leave_out_subjects):\n",
    "            # X_train, y_train = None, None\n",
    "            X_train, y_train = [], []\n",
    "            length_list = []\n",
    "            for subject in subjects:\n",
    "                if subject in leave_out_subjects:\n",
    "                    continue\n",
    "                print('train', subject)\n",
    "                normal_frames = normal_frames_dict[str(subject)]\n",
    "                normal_X = self.split_frames(normal_frames)\n",
    "                normal_y = self.create_labels(len(normal_X), 0)\n",
    "\n",
    "                sleepy_frames = sleepy_frames_dict[str(subject)]\n",
    "                sleepy_X = self.split_frames(sleepy_frames)\n",
    "                sleepy_y = self.create_labels(len(sleepy_X), 1)\n",
    "\n",
    "                X_train.append((normal_X, sleepy_X))\n",
    "                y_train.append((normal_y, sleepy_y))\n",
    "\n",
    "                length_list.append((len(normal_y), len(sleepy_y)))\n",
    "\n",
    "            X_test, y_test = [], []\n",
    "            for subject in leave_out_subjects:\n",
    "                print('test', subject)\n",
    "                normal_frames = normal_frames_dict[str(subject)]\n",
    "                normal_X = self.split_frames(normal_frames)\n",
    "                normal_y = self.create_labels(len(normal_X), 0)\n",
    "\n",
    "                sleepy_frames = sleepy_frames_dict[str(subject)]\n",
    "                sleepy_X = self.split_frames(sleepy_frames)\n",
    "                sleepy_y = self.create_labels(len(sleepy_X), 1)\n",
    "                \n",
    "                X_test.append((normal_X, sleepy_X))\n",
    "                y_test.append((normal_y, sleepy_y))\n",
    "                length_list.append((len(normal_y), len(sleepy_y)))\n",
    "\n",
    "            return X_train, X_test, y_train, y_test, length_list\n",
    "        \n",
    "    def get_shallow_data(self, subjects, leave_out_subjects, segment_length, gru_segment_length):\n",
    "            X_train, y_train = [], []\n",
    "            length_list = []\n",
    "            for sub in subjects:\n",
    "                subject = 'subject' + str(sub)\n",
    "\n",
    "                if sub in leave_out_subjects:\n",
    "                    continue\n",
    "\n",
    "                X_normal, X_sleepy = get_status_rates(subject, 10, segment_length)\n",
    "\n",
    "                X_normal_segmented = split_into_segments(X_normal, gru_segment_length)\n",
    "                X_sleepy_segmented = split_into_segments(X_sleepy, gru_segment_length)\n",
    "\n",
    "                y_normal = self.create_labels(len(X_normal_segmented), 0)\n",
    "                y_sleepy = self.create_labels(len(X_sleepy_segmented), 1)\n",
    "\n",
    "                X_train.append((X_normal_segmented, X_sleepy_segmented))\n",
    "                y_train.append((y_normal, y_sleepy))\n",
    "\n",
    "                length_list.append((len(y_normal), len(y_sleepy)))\n",
    "           \n",
    "            X_test, y_test = [], []\n",
    "            for sub in leave_out_subjects:\n",
    "                subject = 'subject' + str(sub)\n",
    "                print('test', subject)\n",
    "\n",
    "                X_normal, X_sleepy = get_status_rates(subject, 10, segment_length)\n",
    "\n",
    "                X_normal_segmented = split_into_segments(X_normal, gru_segment_length)\n",
    "                X_sleepy_segmented = split_into_segments(X_sleepy, gru_segment_length)\n",
    "\n",
    "                y_normal = self.create_labels(len(X_normal_segmented), 0)\n",
    "                y_sleepy = self.create_labels(len(X_sleepy_segmented), 1)\n",
    "\n",
    "                X_test.append((X_normal_segmented, X_sleepy_segmented))\n",
    "                y_test.append((y_normal, y_sleepy))\n",
    "\n",
    "                length_list.append((len(y_normal), len(y_sleepy)))\n",
    "\n",
    "            return X_train, X_test, y_train, y_test, length_list\n",
    "\n",
    "    def train(self, X_train_shallow, X_train_deep, y_train, num_epochs=10, batch_size=32, validation_split=0.2):\n",
    "        self.batch_size = batch_size\n",
    "        model = self.construct_model()\n",
    "        print('Model is constructed')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "        model.fit([X_train_shallow, X_train_deep], y_train, epochs=num_epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stop])\n",
    "\n",
    "        self.model = model\n",
    "        print(\"Done with training\")\n",
    "        return model\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        model = self.model\n",
    "        return model.evaluate(X_test, y_test, batch_size=self.batch_size)\n",
    "    \n",
    "    def get_f1(self, X_test, y_test):\n",
    "        y_predict = model.predict(X_test, batch_size = self.batch_size)\n",
    "        y_predict_int = np.round(y_predict).astype(int).flatten()\n",
    "        f1 = f1_score(y_test_int, y_predict_int)\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b2e40b-24a3-4b4c-a900-52fe8cccad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded but works (BUT ONLY FOR MULTIPLE TEST SUBJECTS)\n",
    "def equally_sized_lists(array1, array2):\n",
    "    list1 = array1[0]\n",
    "    list2 = array2[0]\n",
    "    list3 = array1[1]\n",
    "    list4 = array2[1]\n",
    "    if len(list1) > len(list2):\n",
    "        list1 = list1[:len(list2)]\n",
    "    elif len(list1) < len(list2):\n",
    "        list2 = list2[:len(list1)]\n",
    "        \n",
    "    if len(list3) > len(list4):\n",
    "        list3 = list3[:len(list4)]\n",
    "    elif len(list3) < len(list4):\n",
    "        list4 = list4[:len(list3)]\n",
    "    return [list1, list3], [list2, list4]\n",
    "\n",
    "def create_equal_arrays(shallow, deep):\n",
    "    first = True\n",
    "    for sub in range(np.array(shallow).shape[0]):\n",
    "        shallow_features = np.array(shallow[sub])\n",
    "        deep_features = np.array(deep[sub])\n",
    "        \n",
    "        shallow_features, deep_features = equally_sized_lists(shallow_features, deep_features)\n",
    "        if first:\n",
    "            normal_shallow = shallow_features[0]\n",
    "            sleepy_shallow = shallow_features[1]\n",
    "\n",
    "            normal_deep = deep_features[0]\n",
    "            sleepy_deep = deep_features[1]\n",
    "            first = False\n",
    "            continue\n",
    "        normal_shallow = np.concatenate((normal_shallow, shallow_features[0]))\n",
    "        sleepy_shallow = np.concatenate((sleepy_shallow, shallow_features[1]))\n",
    "            \n",
    "        normal_deep = np.concatenate((normal_deep, deep_features[0]))\n",
    "        sleepy_deep = np.concatenate((sleepy_deep, deep_features[1]))\n",
    "        \n",
    "    # combine lists and shuffle\n",
    "    shallow = np.concatenate((normal_shallow, sleepy_shallow))\n",
    "    deep = np.concatenate((normal_deep, sleepy_deep))\n",
    "            \n",
    "    return shallow, deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56916945-fea1-4162-a5ff-614b6683aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test subjects:  [20, 23, 11]\n",
      "train 9\n",
      "train 10\n",
      "train 12\n",
      "train 14\n",
      "train 15\n",
      "train 16\n",
      "train 24\n",
      "train 25\n",
      "test 20\n",
      "test 23\n",
      "test 11\n",
      "Starting segmenting normal condition\n",
      "Number of segments 75\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 99\n",
      "Starting segmenting normal condition\n",
      "Number of segments 272\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 340\n",
      "Starting segmenting normal condition\n",
      "Number of segments 292\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 407\n",
      "Starting segmenting normal condition\n",
      "Number of segments 118\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 262\n",
      "Starting segmenting normal condition\n",
      "Number of segments 388\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 377\n",
      "Starting segmenting normal condition\n",
      "Number of segments 320\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 120\n",
      "Starting segmenting normal condition\n",
      "Number of segments 305\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 199\n",
      "Starting segmenting normal condition\n",
      "Number of segments 381\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 335\n",
      "test subject20\n",
      "Starting segmenting normal condition\n",
      "Number of segments 56\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 362\n",
      "test subject23\n",
      "Starting segmenting normal condition\n",
      "Number of segments 216\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 129\n",
      "test subject11\n",
      "Starting segmenting normal condition\n",
      "Number of segments 429\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11231/2327243730.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  for sub in range(np.array(shallow).shape[0]):\n",
      "/tmp/ipykernel_11231/2327243730.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  shallow_features = np.array(shallow[sub])\n",
      "/tmp/ipykernel_11231/2327243730.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  deep_features = np.array(deep[sub])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is obtained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 14:36:57.495849: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 14:36:58.434904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 176 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m y_shallow_test, y_deep_test \u001b[38;5;241m=\u001b[39m create_equal_arrays(y_shallow_test, y_deep_test)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData is obtained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_shallow_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_deep_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_shallow_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mDrowsinessDetector.train\u001b[0;34m(self, X_train_shallow, X_train_deep, y_train, num_epochs, batch_size, validation_split)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train_shallow, X_train_deep, y_train, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m--> 135\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel is constructed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m     early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mDrowsinessDetector.construct_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Define the model with the two input layers and the concatenated output\u001b[39;00m\n\u001b[1;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[shallow_input, deep_input], outputs\u001b[38;5;241m=\u001b[39mdense_2)\n\u001b[0;32m---> 47\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
     ]
    }
   ],
   "source": [
    "detector = DrowsinessDetector(segment_length=60 * 46, gru_segment_length=6)\n",
    "\n",
    "#subjects = [1,3,4,6,7,9,14,15,16,20,23,24]\n",
    "subjects = [9,10, 11, 12, 14,15,16,20,23,24, 25]\n",
    "\n",
    "#leave_out_ratio = 0.3\n",
    "#leave_out_subjects = np.random.choice(subjects, int(len(subjects) * leave_out_ratio), replace=False)\n",
    "leave_out_subjects = [20, 23, 11]\n",
    "print(\"Test subjects: \", leave_out_subjects)\n",
    "\n",
    "normal_dict, sleepy_dict = load_multi_embeddings(subjects)\n",
    "X_deep_train, X_deep_test, y_deep_train, y_deep_test, deep_len_list = detector.get_deep_data(normal_dict, sleepy_dict, subjects, leave_out_subjects)\n",
    "\n",
    "segment_length = 10 * 46\n",
    "gru_segment_length = 6\n",
    "X_shallow_train, X_shallow_test, y_shallow_train, y_shallow_test, shallow_len_list = detector.get_shallow_data(subjects, leave_out_subjects, segment_length, gru_segment_length)\n",
    "\n",
    "X_shallow_train, X_deep_train = create_equal_arrays(X_shallow_train, X_deep_train)\n",
    "y_shallow_train, y_deep_train = create_equal_arrays(y_shallow_train, y_deep_train)\n",
    "\n",
    "X_shallow_test, X_deep_test = create_equal_arrays(X_shallow_test, X_deep_test)\n",
    "y_shallow_test, y_deep_test = create_equal_arrays(y_shallow_test, y_deep_test)\n",
    "print(\"Data is obtained\")\n",
    "model = detector.train(X_shallow_train, X_deep_train, y_shallow_train, num_epochs=100, batch_size=8)\n",
    "#detector.test(X_test, y_test)\n",
    "#y_test, y_predict = detector.compare_predictions(X_test, y_test)\n",
    "#print(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f65a6e6-24ca-4373-9ac3-18b896b24062",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m----> 3\u001b[0m f1 \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mget_f1(\u001b[43mX_test\u001b[49m, y_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(f1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "f1 = detector.get_f1(X_test, y_test)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43381ec-9a89-474a-9ac7-46bbdfdcb4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
