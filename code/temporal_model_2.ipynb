{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6abcbf6-8bef-4423-beb9-960a57dc5d56",
   "metadata": {},
   "source": [
    "# Temporal model Deep GRU + Shallow GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57916f02-9379-471c-b5ee-3c0f6ad50e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Dropout, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a5e8b6b-b209-4e37-8fe4-6f70d479ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe20f1-0864-435b-ae54-3a82af6f6c35",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1960e29b-ff82-4c2f-b56c-329340f82145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(subject):\n",
    "    path = '../embeddings/embeddings_' + subject\n",
    "    normal_embs = np.load(path + '_normal.npy')\n",
    "    sleepy_embs = np.load(path + '_sleepy.npy')\n",
    "\n",
    "    return normal_embs, sleepy_embs\n",
    "\n",
    "def load_multi_embeddings(subjects):\n",
    "    normal_dict = {}\n",
    "    sleepy_dict = {}\n",
    "\n",
    "    for sub in subjects:\n",
    "        path = '../embeddings/embeddings_sub' + str(sub)\n",
    "        normal_frames = np.load(path + '_normal.npy')\n",
    "        sleepy_frames = np.load(path + '_sleepy.npy')\n",
    "\n",
    "        normal_dict[str(sub)] = normal_frames\n",
    "        sleepy_dict[str(sub)] = sleepy_frames\n",
    "\n",
    "    return normal_dict, sleepy_dict\n",
    "\n",
    "from helping_functions import *\n",
    "\n",
    "\n",
    "def get_status_rates(subject, treshhold, segment_length):\n",
    "    status_rates_sleepy, wrong_frames_sleepy = load_blinks(subject, 'sleepy') \n",
    "    status_rates_normal, wrong_frames_normal = load_blinks(subject, 'normal') \n",
    "    \n",
    "    print(\"Starting segmenting normal condition\")\n",
    "    blink_counts_normal, average_durs_normal = run_analysis(status_rates_normal, wrong_frames_normal, treshhold, segment_length)\n",
    "    print(\"Starting segmenting sleepy condition\")\n",
    "    blink_counts_sleepy, average_durs_sleepy = run_analysis(status_rates_sleepy, wrong_frames_sleepy, treshhold, segment_length)\n",
    "    \n",
    "    return list(zip(blink_counts_normal, average_durs_normal)), list(zip(blink_counts_sleepy, average_durs_sleepy))\n",
    "\n",
    "def split_into_segments(frames, gru_segment_length):\n",
    "    num_segments = len(frames) // gru_segment_length\n",
    "    frames = frames[:num_segments * gru_segment_length]\n",
    "    return np.array(np.split(np.array(frames), num_segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd569b8d-a68b-4050-b255-ae890bcc3e6f",
   "metadata": {},
   "source": [
    "## First model: GRU deep features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628ea07-9570-41ec-85c7-2df0cd68942b",
   "metadata": {},
   "source": [
    "### Model initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630324e1-d991-4099-8a21-e7e74a06b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78e999b-ba8c-49d6-bcb2-c1f083806e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Input, concatenate\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "class DrowsinessDetector:\n",
    "    def __init__(self, segment_length, gru_segment_length):\n",
    "        self.segment_length = segment_length\n",
    "        self.deep_input_shape = (self.segment_length, 2048)\n",
    "        self.shallow_input_shape = (gru_segment_length, 2)\n",
    "        \n",
    "    def split_frames(self, frames):\n",
    "        num_segments = len(frames) // self.segment_length\n",
    "        frames = frames[:num_segments * self.segment_length]\n",
    "        return np.array(np.split(frames, num_segments))\n",
    "    \n",
    "    def create_labels(self, num_segments, label):\n",
    "        return np.array([label] * num_segments)\n",
    "    \n",
    "    def shuffle_data(self, X_shallow, X_deep, y):\n",
    "        indices = np.arange(len(X_shallow))\n",
    "        np.random.shuffle(indices)\n",
    "        return X_shallow[indices], X_deep[indices], y[indices]\n",
    "\n",
    "    def construct_model(self):\n",
    "        # Define the shallow input layer and GRU\n",
    "        shallow_input = Input(shape=self.shallow_input_shape)\n",
    "        shallow_gru = SimpleRNN(32)(shallow_input)\n",
    "\n",
    "        # Define the deep input layer and GRU\n",
    "        deep_input = Input(shape=self.deep_input_shape)\n",
    "        deep_gru = SimpleRNN(128)(deep_input)\n",
    "\n",
    "        # Concatenate the outputs of the two GRUs\n",
    "        merged = concatenate([shallow_gru, deep_gru])\n",
    "\n",
    "        # Add a dense layer to control how much focus is put on the shallow and deep features\n",
    "        dense_1 = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(merged)\n",
    "\n",
    "        # Add dropout layer\n",
    "        dropout_1 = Dropout(0.5)(dense_1)\n",
    "\n",
    "        # Add a dense layer for the final prediction\n",
    "        dense_2 = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(dropout_1)\n",
    "\n",
    "        # Define the model with the two input layers and the concatenated output\n",
    "        model = Model(inputs=[shallow_input, deep_input], outputs=dense_2)\n",
    "\n",
    "        optimizer = Adam(lr=0.0001)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_deep_data(self, normal_frames_dict, sleepy_frames_dict, subjects, leave_out_subjects):\n",
    "            # X_train, y_train = None, None\n",
    "            X_train, y_train = [], []\n",
    "            length_list = []\n",
    "            for subject in subjects:\n",
    "                if subject in leave_out_subjects:\n",
    "                    continue\n",
    "                print('train', subject)\n",
    "                normal_frames = normal_frames_dict[str(subject)]\n",
    "                normal_X = self.split_frames(normal_frames)\n",
    "                normal_y = self.create_labels(len(normal_X), 0)\n",
    "\n",
    "                sleepy_frames = sleepy_frames_dict[str(subject)]\n",
    "                sleepy_X = self.split_frames(sleepy_frames)\n",
    "                sleepy_y = self.create_labels(len(sleepy_X), 1)\n",
    "\n",
    "                X_train.append((normal_X, sleepy_X))\n",
    "                y_train.append((normal_y, sleepy_y))\n",
    "\n",
    "                length_list.append((len(normal_y), len(sleepy_y)))\n",
    "\n",
    "            X_test, y_test = [], []\n",
    "            for subject in leave_out_subjects:\n",
    "                print('test', subject)\n",
    "                normal_frames = normal_frames_dict[str(subject)]\n",
    "                normal_X = self.split_frames(normal_frames)\n",
    "                normal_y = self.create_labels(len(normal_X), 0)\n",
    "\n",
    "                sleepy_frames = sleepy_frames_dict[str(subject)]\n",
    "                sleepy_X = self.split_frames(sleepy_frames)\n",
    "                sleepy_y = self.create_labels(len(sleepy_X), 1)\n",
    "                \n",
    "                X_test.append((normal_X, sleepy_X))\n",
    "                y_test.append((normal_y, sleepy_y))\n",
    "                length_list.append((len(normal_y), len(sleepy_y)))\n",
    "\n",
    "            return X_train, X_test, y_train, y_test, length_list\n",
    "        \n",
    "    def get_shallow_data(self, subjects, leave_out_subjects, segment_length, gru_segment_length):\n",
    "            X_train, y_train = [], []\n",
    "            length_list = []\n",
    "            for sub in subjects:\n",
    "                subject = 'subject' + str(sub)\n",
    "\n",
    "                if sub in leave_out_subjects:\n",
    "                    continue\n",
    "\n",
    "                X_normal, X_sleepy = get_status_rates(subject, 10, segment_length)\n",
    "\n",
    "                X_normal_segmented = split_into_segments(X_normal, gru_segment_length)\n",
    "                X_sleepy_segmented = split_into_segments(X_sleepy, gru_segment_length)\n",
    "\n",
    "                y_normal = self.create_labels(len(X_normal_segmented), 0)\n",
    "                y_sleepy = self.create_labels(len(X_sleepy_segmented), 1)\n",
    "\n",
    "                X_train.append((X_normal_segmented, X_sleepy_segmented))\n",
    "                y_train.append((y_normal, y_sleepy))\n",
    "\n",
    "                length_list.append((len(y_normal), len(y_sleepy)))\n",
    "           \n",
    "            X_test, y_test = [], []\n",
    "            for sub in leave_out_subjects:\n",
    "                subject = 'subject' + str(sub)\n",
    "                print('test', subject)\n",
    "\n",
    "                X_normal, X_sleepy = get_status_rates(subject, 10, segment_length)\n",
    "\n",
    "                X_normal_segmented = split_into_segments(X_normal, gru_segment_length)\n",
    "                X_sleepy_segmented = split_into_segments(X_sleepy, gru_segment_length)\n",
    "\n",
    "                y_normal = self.create_labels(len(X_normal_segmented), 0)\n",
    "                y_sleepy = self.create_labels(len(X_sleepy_segmented), 1)\n",
    "\n",
    "                X_test.append((X_normal_segmented, X_sleepy_segmented))\n",
    "                y_test.append((y_normal, y_sleepy))\n",
    "\n",
    "                length_list.append((len(y_normal), len(y_sleepy)))\n",
    "\n",
    "            return X_train, X_test, y_train, y_test, length_list\n",
    "\n",
    "    def train(self, X_train_shallow, X_train_deep, y_train, num_epochs=10, batch_size=32, validation_split=0.2):\n",
    "        self.batch_size = batch_size\n",
    "        model = self.construct_model()\n",
    "        print('Model is constructed')\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\n",
    "\n",
    "        model.fit([X_train_shallow, X_train_deep], y_train, epochs=num_epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stop])\n",
    "\n",
    "        self.model = model\n",
    "        print(\"Done with training\")\n",
    "        return model\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        model = self.model\n",
    "        return model.evaluate(X_test, y_test, batch_size=self.batch_size)\n",
    "    \n",
    "    def get_f1(self, X_test_shallow, X_test_deep, y_test):\n",
    "        y_predict = model.predict([X_test_shallow, X_test_deep], batch_size = self.batch_size)\n",
    "        y_predict_int = np.round(y_predict).astype(int).flatten()\n",
    "        f1 = f1_score(y_test, y_predict_int)\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b2e40b-24a3-4b4c-a900-52fe8cccad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded but works (BUT ONLY FOR MULTIPLE TEST SUBJECTS)\n",
    "def equally_sized_lists(array1, array2):\n",
    "    list1 = array1[0]\n",
    "    list2 = array2[0]\n",
    "    list3 = array1[1]\n",
    "    list4 = array2[1]\n",
    "    if len(list1) > len(list2):\n",
    "        list1 = list1[:len(list2)]\n",
    "    elif len(list1) < len(list2):\n",
    "        list2 = list2[:len(list1)]\n",
    "        \n",
    "    if len(list3) > len(list4):\n",
    "        list3 = list3[:len(list4)]\n",
    "    elif len(list3) < len(list4):\n",
    "        list4 = list4[:len(list3)]\n",
    "    return [list1, list3], [list2, list4]\n",
    "\n",
    "def create_equal_arrays(shallow, deep):\n",
    "    first = True\n",
    "    for sub in range(np.array(shallow).shape[0]):\n",
    "        shallow_features = np.array(shallow[sub])\n",
    "        deep_features = np.array(deep[sub])\n",
    "        \n",
    "        shallow_features, deep_features = equally_sized_lists(shallow_features, deep_features)\n",
    "        if first:\n",
    "            normal_shallow = shallow_features[0]\n",
    "            sleepy_shallow = shallow_features[1]\n",
    "\n",
    "            normal_deep = deep_features[0]\n",
    "            sleepy_deep = deep_features[1]\n",
    "            first = False\n",
    "            continue\n",
    "        normal_shallow = np.concatenate((normal_shallow, shallow_features[0]))\n",
    "        sleepy_shallow = np.concatenate((sleepy_shallow, shallow_features[1]))\n",
    "            \n",
    "        normal_deep = np.concatenate((normal_deep, deep_features[0]))\n",
    "        sleepy_deep = np.concatenate((sleepy_deep, deep_features[1]))\n",
    "        \n",
    "    # combine list\n",
    "    shallow = np.concatenate((normal_shallow, sleepy_shallow))\n",
    "    deep = np.concatenate((normal_deep, sleepy_deep))\n",
    "            \n",
    "    return shallow, deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56916945-fea1-4162-a5ff-614b6683aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test subjects:  [20, 23, 11]\n",
      "train 9\n",
      "train 10\n",
      "train 12\n",
      "train 14\n",
      "train 15\n",
      "train 16\n",
      "train 24\n",
      "train 25\n",
      "test 20\n",
      "test 23\n",
      "test 11\n",
      "Starting segmenting normal condition\n",
      "Number of segments 75\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 99\n",
      "Starting segmenting normal condition\n",
      "Number of segments 272\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 340\n",
      "Starting segmenting normal condition\n",
      "Number of segments 292\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 407\n",
      "Starting segmenting normal condition\n",
      "Number of segments 118\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 262\n",
      "Starting segmenting normal condition\n",
      "Number of segments 388\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 377\n",
      "Starting segmenting normal condition\n",
      "Number of segments 320\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 120\n",
      "Starting segmenting normal condition\n",
      "Number of segments 305\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 199\n",
      "Starting segmenting normal condition\n",
      "Number of segments 381\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 335\n",
      "test subject20\n",
      "Starting segmenting normal condition\n",
      "Number of segments 56\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 362\n",
      "test subject23\n",
      "Starting segmenting normal condition\n",
      "Number of segments 216\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 129\n",
      "test subject11\n",
      "Starting segmenting normal condition\n",
      "Number of segments 429\n",
      "Starting segmenting sleepy condition\n",
      "Number of segments 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18274/1903579072.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  for sub in range(np.array(shallow).shape[0]):\n",
      "/tmp/ipykernel_18274/1903579072.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  shallow_features = np.array(shallow[sub])\n",
      "/tmp/ipykernel_18274/1903579072.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  deep_features = np.array(deep[sub])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is obtained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 17:26:07.824147: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 17:26:08.796029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30494 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0\n",
      "/opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is constructed\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 95s 1s/step - loss: 1.0366 - accuracy: 0.6885 - val_loss: 0.8188 - val_accuracy: 0.9206\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 86s 1s/step - loss: 0.7749 - accuracy: 0.9405 - val_loss: 0.7103 - val_accuracy: 0.9762\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 86s 1s/step - loss: 0.6686 - accuracy: 0.9762 - val_loss: 0.6327 - val_accuracy: 0.9921\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 83s 1s/step - loss: 0.5942 - accuracy: 0.9901 - val_loss: 0.5795 - val_accuracy: 0.9921\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 84s 1s/step - loss: 0.5449 - accuracy: 0.9960 - val_loss: 0.5409 - val_accuracy: 0.9921\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 84s 1s/step - loss: 0.5030 - accuracy: 0.9980 - val_loss: 0.5090 - val_accuracy: 0.9921\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 84s 1s/step - loss: 0.4705 - accuracy: 0.9980 - val_loss: 0.4818 - val_accuracy: 0.9921\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 82s 1s/step - loss: 0.4440 - accuracy: 1.0000 - val_loss: 0.4556 - val_accuracy: 0.9921\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 84s 1s/step - loss: 0.4212 - accuracy: 1.0000 - val_loss: 0.4327 - val_accuracy: 0.9921\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 84s 1s/step - loss: 0.4014 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 0.9921\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 83s 1s/step - loss: 0.3844 - accuracy: 1.0000 - val_loss: 0.3999 - val_accuracy: 0.9921\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 83s 1s/step - loss: 0.3685 - accuracy: 1.0000 - val_loss: 0.3877 - val_accuracy: 0.9921\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 86s 1s/step - loss: 0.3538 - accuracy: 1.0000 - val_loss: 0.3750 - val_accuracy: 0.9921\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.3401 - accuracy: 1.0000 - val_loss: 0.3583 - val_accuracy: 0.9921\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.3268 - accuracy: 1.0000 - val_loss: 0.3485 - val_accuracy: 0.9921\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.3142 - accuracy: 1.0000 - val_loss: 0.3344 - val_accuracy: 0.9921\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.3023 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 0.9921\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.2909 - accuracy: 1.0000 - val_loss: 0.3119 - val_accuracy: 0.9921\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.2799 - accuracy: 1.0000 - val_loss: 0.3010 - val_accuracy: 0.9921\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.2692 - accuracy: 1.0000 - val_loss: 0.2896 - val_accuracy: 0.9921\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 86s 1s/step - loss: 0.2589 - accuracy: 1.0000 - val_loss: 0.2802 - val_accuracy: 0.9921\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.2490 - accuracy: 1.0000 - val_loss: 0.2692 - val_accuracy: 0.9921\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 89s 1s/step - loss: 0.2394 - accuracy: 1.0000 - val_loss: 0.2591 - val_accuracy: 0.9921\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 86s 1s/step - loss: 0.2300 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9921\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 85s 1s/step - loss: 0.2209 - accuracy: 1.0000 - val_loss: 0.2412 - val_accuracy: 0.9921\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.2121 - accuracy: 1.0000 - val_loss: 0.2309 - val_accuracy: 0.9921\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 86s 1s/step - loss: 0.2036 - accuracy: 1.0000 - val_loss: 0.2226 - val_accuracy: 0.9921\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 86s 1s/step - loss: 0.1953 - accuracy: 1.0000 - val_loss: 0.2140 - val_accuracy: 0.9921\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.1873 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.9921\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 89s 1s/step - loss: 0.1796 - accuracy: 1.0000 - val_loss: 0.1978 - val_accuracy: 0.9921\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 84s 1s/step - loss: 0.1722 - accuracy: 1.0000 - val_loss: 0.1897 - val_accuracy: 0.9921\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 87s 1s/step - loss: 0.1650 - accuracy: 1.0000 - val_loss: 0.1826 - val_accuracy: 0.9921\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 89s 1s/step - loss: 0.1581 - accuracy: 1.0000 - val_loss: 0.1756 - val_accuracy: 0.9921\n",
      "Epoch 34/100\n",
      "12/63 [====>.........................] - ETA: 1:06 - loss: 0.1537 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m X_shallow_test, X_deep_test, y_test \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mshuffle_data(X_shallow_test, X_deep_test, y_shallow_test)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData is obtained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_shallow_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_deep_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mDrowsinessDetector.train\u001b[0;34m(self, X_train_shallow, X_train_deep, y_train, num_epochs, batch_size, validation_split)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel is constructed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_shallow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_deep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone with training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1212\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1213\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1214\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1215\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1216\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1218\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    907\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 910\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    913\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    940\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    941\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3128\u001b[0m   (graph_function,\n\u001b[1;32m   3129\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1955\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1958\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1961\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m     args,\n\u001b[1;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1964\u001b[0m     executing_eagerly)\n\u001b[1;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    607\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    611\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detector = DrowsinessDetector(segment_length=60 * 46, gru_segment_length=6)\n",
    "\n",
    "#subjects = [1,3,4,6,7,9,14,15,16,20,23,24]\n",
    "subjects = [9,10, 11, 12, 14,15,16,20,23,24, 25]\n",
    "\n",
    "#leave_out_ratio = 0.3\n",
    "#leave_out_subjects = np.random.choice(subjects, int(len(subjects) * leave_out_ratio), replace=False)\n",
    "leave_out_subjects = [20, 23, 11]\n",
    "print(\"Test subjects: \", leave_out_subjects)\n",
    "\n",
    "normal_dict, sleepy_dict = load_multi_embeddings(subjects)\n",
    "X_deep_train, X_deep_test, y_deep_train, y_deep_test, deep_len_list = detector.get_deep_data(normal_dict, sleepy_dict, subjects, leave_out_subjects)\n",
    "\n",
    "segment_length = 10 * 46\n",
    "gru_segment_length = 6\n",
    "X_shallow_train, X_shallow_test, y_shallow_train, y_shallow_test, shallow_len_list = detector.get_shallow_data(subjects, leave_out_subjects, segment_length, gru_segment_length)\n",
    "\n",
    "X_shallow_train, X_deep_train = create_equal_arrays(X_shallow_train, X_deep_train)\n",
    "y_shallow_train, y_deep_train = create_equal_arrays(y_shallow_train, y_deep_train)\n",
    "\n",
    "X_shallow_test, X_deep_test = create_equal_arrays(X_shallow_test, X_deep_test)\n",
    "y_shallow_test, y_deep_test = create_equal_arrays(y_shallow_test, y_deep_test)\n",
    "\n",
    "X_shallow_train, X_deep_train, y_train = detector.shuffle_data(X_shallow_train, X_deep_train, y_shallow_train)\n",
    "X_shallow_test, X_deep_test, y_test = detector.shuffle_data(X_shallow_test, X_deep_test, y_shallow_test)\n",
    "\n",
    "print(\"Data is obtained\")\n",
    "model = detector.train(X_shallow_train, X_deep_train, y_train, num_epochs=100, batch_size=8)\n",
    "#detector.test(X_test, y_test)\n",
    "#y_test, y_predict = detector.compare_predictions(X_test, y_test)\n",
    "#print(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f65a6e6-24ca-4373-9ac3-18b896b24062",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m----> 3\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_f1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_shallow_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_deep_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(f1)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mDrowsinessDetector.get_f1\u001b[0;34m(self, X_test_shallow, X_test_deep, y_test)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_f1\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_test_shallow, X_test_deep, y_test):\n\u001b[0;32m--> 152\u001b[0m     y_predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict([X_test_shallow, X_test_deep], batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m    153\u001b[0m     y_predict_int \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(y_predict)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    154\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(y_test, y_predict_int)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "f1 = detector.get_f1(X_shallow_test, X_deep_test, y_test)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5736f7-075f-4286-a467-e15de9f66783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
