{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbea404f-247d-47f9-b312-52c4df5f8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import cv2\n",
    "import dlib\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580ab404-9ba9-4dcd-93b2-8ed508265e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Displayinh the number of GPUs available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93c6702-aded-4f9d-89c6-8457564a4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9c6af2-c6ce-47ab-9ed8-9b8096454a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 13:01:50.673741: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 13:01:52.101914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 29477 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94699520/94694792 [==============================] - 3s 0us/step\n",
      "94707712/94694792 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c967b007-2b35-46ba-bae5-f4fb008eb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a test image, resize it, and convert it to grayscale\n",
    "def load_image(path, show=False, gray_show=False):\n",
    "    test_image = cv2.imread(path)\n",
    "    test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    test_image = imutils.resize(test_image, width=500)\n",
    "    gray = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(test_image)\n",
    "        plt.show()\n",
    "        if gray_show:\n",
    "            plt.imshow(gray, cmap='gray')\n",
    "            plt.show()\n",
    "    return test_image, gray\n",
    "\n",
    "def detect_faces(face_detector, gray, img, scale_factor, frame_num, show_multi_faces = False):\n",
    "    detections = haar_cascade.detectMultiScale(gray, scaleFactor=scale_factor, minNeighbors=5, minSize=(150, 150), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    #Checking if there are more than 1 detected faces\n",
    "    if len(detections) > 1:\n",
    "        print('multiple faces', frame_num)\n",
    "        if detections[0][2] > detections[1][2]:\n",
    "            face = detections[1]\n",
    "            other = detections[0]\n",
    "        else:\n",
    "            face = detections[0]\n",
    "            other = detections[1]\n",
    "        if show_multi_faces:\n",
    "            print('largest and chosen face')\n",
    "            fX, fY, fW, fH = face[0],face[1], face[2], face[3]\n",
    "            plt.imshow(gray[fY:fY + fH, fX:fX + fW], cmap='gray')\n",
    "            plt.show()\n",
    "            print('not chosen face')\n",
    "            fX, fY, fW, fH = other[0], other[1], other[2], other[3]\n",
    "            plt.imshow(gray[fY:fY + fH, fX:fX + fW], cmap='gray')\n",
    "            plt.show()\n",
    "    elif len(detections) == 1:\n",
    "        face = detections[0]\n",
    "    # if no face detected\n",
    "    elif len(detections) == 0:\n",
    "        print('No face detected', frame_num)\n",
    "        return False\n",
    "    fX, fY, fW, fH = face[0], face[1], face[2], face[3]\n",
    "    face_img = Image.fromarray(img[fY:fY + fH, fX:fX + fW])\n",
    "    face_img = face_img.resize((224, 224))\n",
    "    #face_img.show()\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0a5c9b9-676e-4948-9e43-8108b368202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify subject and condition\n",
    "subject = 'P11407'\n",
    "condition = 'normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2027a291-bdbc-4900-9436-59407f865f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# haar Cascade path\n",
    "cascade_path = '../models/haarcascade_frontalface_default.xml'\n",
    "# create haar cascade\n",
    "haar_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "face_detector = 'haar'\n",
    "\n",
    "frame_start = 35000\n",
    "frame_end =  203000\n",
    "frames = range(frame_start, frame_end + 1)\n",
    "\n",
    "scale_factor = 1.03\n",
    "\n",
    "folder_path = '../frames/' + subject + '/' + condition + '/frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfda09-162b-4923-b4b2-bf6ff6e79fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(frames, folder_path, face_detector, model, scale_factor, show=False, show_multi_faces=False):\n",
    "    no_face_count = 0\n",
    "    embeddings = []\n",
    "    for frame in frames:\n",
    "        if frame % 1000 == 0:\n",
    "            print(frame)\n",
    "            gc.collect()\n",
    "        if frame == 100000:\n",
    "            np.save('embeddings_sub' + subject[-1] + '_' + condition + '_temp1.npy', embeddings)\n",
    "        if frame == 135000:\n",
    "            np.save('embeddings_sub' + subject[-1] + '_' + condition + '_temp2.npy', embeddings)\n",
    "        if frame == 160000:\n",
    "            np.save('embeddings_sub' + subject[-1] + '_' + condition + '_temp3.npy', embeddings)\n",
    "        path = folder_path + str(frame) + '.jpg'\n",
    "        test_img, gray = load_image(path, show=False)\n",
    "        face = detect_faces(face_detector, gray, test_img, scale_factor, frame, show_multi_faces=False)\n",
    "\n",
    "        if not face:\n",
    "            print(\"No face detected\", frame)\n",
    "            no_face_count += 1\n",
    "            continue\n",
    "\n",
    "        face_array = np.asarray(face)\n",
    "        pixels = face_array.astype('float32')\n",
    "        samples = np.expand_dims(pixels, axis=0)\n",
    "        samples = preprocess_input(samples, version=2)\n",
    "        embedding = model.predict(samples, verbose=0)\n",
    "        embeddings.append(embedding[0])\n",
    "    print(no_face_count)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings(frames, folder_path, face_detector, model, scale_factor)\n",
    "np.save('embeddings_sub' + subject[-1] + '_' + condition + '.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d03a49-f012-4ff9-87cf-ae1799da9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts average embedding in a segment from given embeddigs\n",
    "def embeddings_segment(embeddings, video_len, segment_len):\n",
    "    # The amount of frames at the end that are not taken into account\n",
    "    rest = video_len % segment_len\n",
    "    num_frames = video_len - rest\n",
    "    avg_embeddings = []\n",
    "    acum_embeddings = np.zeros(2048)\n",
    "    # a blink is counted to a segment,when the blink starts in that segment\n",
    "    for frame in range(num_frames + 1):\n",
    "        if frame % 1000 == 0:\n",
    "            print(frame)\n",
    "        acum_embeddings = acum_embeddings + np.array(embeddings[frame])\n",
    "        # only happens at the end of a segment\n",
    "        if frame % segment_len == 0 and frame != 0:\n",
    "            avg_embeddings.append(acum_embeddings / segment_len)\n",
    "            acum_embeddings = np.zeros(2048)\n",
    "            #print('New segment: ', frame)\n",
    "    return avg_embeddings\n",
    "\n",
    "avg_embeddings = embeddings_segment(embeddings, len(embeddings), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd135a6-fedb-46be-8894-6749d1823265",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pri_comps = pca.fit_transform(avg_embeddings)\n",
    "\n",
    "print(pri_comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7b593-a2c6-4ce2-9f4a-ca0e7149e5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
