{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Analysis\n",
    "This notebook contains the code for segmenting, visualizing and analysing the deep features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(subject):\n",
    "    path = '../embeddings/embeddings_' + subject\n",
    "    normal_embs = np.load(path + '_normal.npy')\n",
    "    sleepy_embs = np.load(path + '_sleepy.npy')\n",
    "\n",
    "    return normal_embs, sleepy_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracts average embedding in a segment from given embeddigs\n",
    "def embeddings_segment(embeddings, video_len, segment_len):\n",
    "    # The amount of frames at the end that are not taken into account\n",
    "    rest = video_len % segment_len\n",
    "    num_frames = video_len - rest\n",
    "    avg_embeddings = []\n",
    "    acum_embeddings = np.zeros(2048)\n",
    "    # a blink is counted to a segment,when the blink starts in that segment\n",
    "    for frame in range(num_frames + 1):\n",
    "        # if frame % 1000 == 0:\n",
    "        #     print(frame)\n",
    "        acum_embeddings = acum_embeddings + np.array(embeddings[frame])\n",
    "        # only happens at the end of a segment\n",
    "        if frame % segment_len == 0 and frame != 0:\n",
    "            avg_embeddings.append(acum_embeddings / segment_len)\n",
    "            acum_embeddings = np.zeros(2048)\n",
    "            #print('New segment: ', frame)\n",
    "    normalized_embeddings = preprocessing.normalize(avg_embeddings, norm='l2')\n",
    "    return normalized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub23'\n",
    "norm_embs, sleep_embs = load_embeddings(subject)\n",
    "\n",
    "print(len(norm_embs), len(sleep_embs))\n",
    "\n",
    "segment_length = int(46 * 60 * 1)\n",
    "avg_norm_embs = embeddings_segment(norm_embs, len(norm_embs), segment_length)\n",
    "avg_sleep_embs = embeddings_segment(sleep_embs, len(sleep_embs), segment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_len = len(avg_norm_embs)\n",
    "sleep_len = len(avg_sleep_embs)\n",
    "all_embs = list(avg_norm_embs) + list(avg_sleep_embs)\n",
    "\n",
    "print(norm_len, sleep_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pri_comps = pca.fit_transform(all_embs)\n",
    "var = pca.explained_variance_ratio_\n",
    "print('Percentage of variance explained by the two components: ', float(sum(var)))\n",
    "pc_normal = pri_comps[:norm_len]\n",
    "pc_sleep = pri_comps[norm_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pc_normal[:,0], pc_normal[:,1], label = 'Normal')\n",
    "plt.scatter(pc_sleep[:,0], pc_sleep[:,1], label = 'Sleep Restricted')\n",
    "#plt.scatter(pri_comps[:,0], pri_comps[:,1])\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('The first two principal components of 1 min segments of subject '+ subject[-2:])\n",
    "plt.legend()\n",
    "plt.savefig('../final_figures/pc_plot_subject'+ subject[-1] + '.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Check Wilcoxon across all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def wilcoxon_test(normal_vals, sleepy_vals):\n",
    "    w_score = stats.wilcoxon(normal_vals, sleepy_vals[:len(normal_vals)])\n",
    "    return w_score[0], w_score[1]\n",
    "\n",
    "\n",
    "def equally_sized_lists(list1, list2):\n",
    "    if len(list1) > len(list2):\n",
    "        list1 = list1[:len(list2)]\n",
    "    elif len(list1) < len(list2):\n",
    "        list2 = list2[:len(list1)]\n",
    "    return list1, list2\n",
    "\n",
    "def wilcoxon_check(subjects):\n",
    "    sum_score = 0\n",
    "    for sub in subjects:\n",
    "        subject = 'sub' + str(sub)\n",
    "        norm_embs, sleep_embs = load_embeddings(subject)\n",
    "        avg_norm_embs = embeddings_segment(norm_embs, len(norm_embs), 46 * 60)\n",
    "        avg_sleep_embs = embeddings_segment(sleep_embs, len(sleep_embs), 46 * 60)\n",
    "        norm_len = len(avg_norm_embs)\n",
    "\n",
    "        all_embs = list(avg_norm_embs) + list(avg_sleep_embs)\n",
    "        pca = PCA(n_components=2)\n",
    "        pri_comps = pca.fit_transform(all_embs)\n",
    "\n",
    "        pc_normal = pri_comps[:norm_len]\n",
    "        pc_sleepy = pri_comps[norm_len:]\n",
    "\n",
    "        pc_norm, pc_sleep = equally_sized_lists(np.concatenate((pc_normal[:,0], pc_normal[:,1])), np.concatenate((pc_sleepy[:,0], pc_sleepy[:,1])))\n",
    "\n",
    "        w_score = wilcoxon_test(pc_norm, pc_sleep)\n",
    "        sum_score += w_score[1]\n",
    "        \n",
    "        print(\"Done with subject \" + str(sub))\n",
    "\n",
    "    return (sum_score / len(subjects))\n",
    "\n",
    "subjects = [1,3,4,6,7,9,10,11,12,14,15,16,20,22,23,24,25]\n",
    "avg_wil = wilcoxon_check(subjects)\n",
    "print(\"Average wilcoxon p-value for the first two principal components combined: \", avg_wil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Check PCA Var across all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_var_check(subjects):\n",
    "    sum_var = 0\n",
    "    for sub in subjects:\n",
    "        subject = 'sub' + str(sub)\n",
    "        norm_embs, sleep_embs = load_embeddings(subject)\n",
    "        avg_norm_embs = embeddings_segment(norm_embs, len(norm_embs), 46 * 60)\n",
    "        avg_sleep_embs = embeddings_segment(sleep_embs, len(sleep_embs), 46 * 60)\n",
    "        all_embs = list(avg_norm_embs) + list(avg_sleep_embs)\n",
    "        pca = PCA(n_components=9)\n",
    "        pri_comps = pca.fit_transform(all_embs)\n",
    "        var = pca.explained_variance_ratio_\n",
    "        sum_var += sum(var)\n",
    "    return sum_var / len(subjects)\n",
    "\n",
    "subjects = [1,3,4,6,7,9,10,11,12,14,15,16,20,22,23,24,25]\n",
    "avg_var = pca_var_check(subjects)\n",
    "print(\"Average percentage of captured variance across all subjects: \", int(avg_var * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_model(X_normal, X_sleepy):\n",
    "    y_normal = list(np.zeros(len(X_normal)))\n",
    "    y_sleepy = list(np.ones(len(X_sleepy)))\n",
    "\n",
    "    X = X_normal + X_sleepy\n",
    "    y = y_normal + y_sleepy\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_norm = scaler.fit_transform(X)\n",
    "    #print(\"Maximum values :\", scaler.data_max_)\n",
    "    #print(\"Minimum values :\", scaler.data_min_)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.3,random_state=42)\n",
    "    \n",
    "    # train model\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_round =  round(f1, 2) \n",
    "    print(\"F1 score: \", f1_round)\n",
    "\n",
    "\n",
    "    return model, f1_round, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_normal = list(zip(pc_normal[:,0], pc_normal[:,1]))\n",
    "X_sleepy = list(zip(pc_sleep[:,0], pc_sleep[:,1]))\n",
    "\n",
    "model, f1_round, X_train, X_test, y_train, y_test = train_model(X_normal, X_sleepy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_subject_seperately(subjects, seg_len_list):\n",
    "    avg_f1_scores = []\n",
    "    for seg_len in seg_len_list:\n",
    "        seg_len = int(46 * 60 * seg_len)\n",
    "        f1_scores = []\n",
    "        for sub in subjects:\n",
    "            subject = 'sub' + str(sub)\n",
    "            norm_embs, sleep_embs = load_embeddings(subject)\n",
    "            avg_norm_embs = embeddings_segment(norm_embs, len(norm_embs), seg_len)\n",
    "            avg_sleep_embs = embeddings_segment(sleep_embs, len(sleep_embs), seg_len)\n",
    "            norm_len = len(avg_norm_embs)\n",
    "            sleep_len = len(avg_sleep_embs)\n",
    "            all_embs = list(avg_norm_embs) + list(avg_sleep_embs)\n",
    "            pca = PCA(n_components=2)\n",
    "            pri_comps = pca.fit_transform(all_embs)\n",
    "            pc_normal = pri_comps[:norm_len]\n",
    "            pc_sleepy = pri_comps[norm_len:]\n",
    "\n",
    "            X_normal = list(zip(pc_normal[:,0], pc_normal[:,1]))\n",
    "            X_sleepy = list(zip(pc_sleepy[:,0], pc_sleepy[:,1]))\n",
    "\n",
    "            model, f1_round, X_train, X_test, y_train, y_test = train_model(X_normal, X_sleepy)\n",
    "\n",
    "            f1_scores.append(f1_round)\n",
    "\n",
    "            print(\"Subject \" + str(sub)+ \" is done\")\n",
    "        print(\"Average F1 score of segment length \" + str(seg_len), sum(f1_scores) / len(f1_scores))\n",
    "        avg_f1_scores.append(sum(f1_scores) / len(f1_scores))\n",
    "    return avg_f1_scores\n",
    "# 6,9 16, 20 dont have enough sleepy segments for seg len is 5\n",
    "subjects = [1,3,4,10,11,12,14,15,22,23,24]\n",
    "\n",
    "seg_len_list = [0.11, 0.5, 1, 2, 5]\n",
    "avg_f1_scores = class_subject_seperately(subjects, seg_len_list)\n",
    "print(avg_f1_scores)\n",
    "\n",
    "x_values = range(len(seg_len_list))\n",
    "plt.plot(x_values, avg_f1_scores)\n",
    "plt.xticks(x_values, seg_len_list)\n",
    "plt.xlabel('Segment length (in minutes)')\n",
    "plt.ylabel('F1 score')\n",
    "plt.title('Deep classification performance for all subjects when increasing segment length')\n",
    "plt.savefig('../final_figures/deep_classification.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def obtain_data(subjects, pca):\n",
    "    seg_len = int(46 * 60 * 1)\n",
    "    first = True\n",
    "    for sub in subjects:\n",
    "        subject = 'sub' + sub\n",
    "        if first:\n",
    "            norm_embs, sleep_embs = load_embeddings(subject)\n",
    "            avg_norm_embs = embeddings_segment(norm_embs, len(norm_embs), seg_len)\n",
    "            avg_sleep_embs = embeddings_segment(sleep_embs, len(sleep_embs), seg_len)\n",
    "            if pca:\n",
    "                norm_len = len(avg_norm_embs)\n",
    "                sleep_len = len(avg_sleep_embs)\n",
    "                all_embs = list(avg_norm_embs) + list(avg_sleep_embs)\n",
    "                pca = PCA(n_components=3)\n",
    "                pri_comps = pca.fit_transform(all_embs)\n",
    "                pc_normal_train = pri_comps[:norm_len]\n",
    "                pc_sleepy_train = pri_comps[norm_len:]\n",
    "                first = False\n",
    "            else:\n",
    "                pc_normal_train = avg_norm_embs\n",
    "                pc_sleepy_train = avg_sleep_embs\n",
    "                first = False\n",
    "        else:\n",
    "            norm_embs, sleep_embs = load_embeddings(subject)\n",
    "            avg_norm_embs = embeddings_segment(norm_embs, len(norm_embs), seg_len)\n",
    "            avg_sleep_embs = embeddings_segment(sleep_embs, len(sleep_embs), seg_len)\n",
    "            if pca:\n",
    "                norm_len = len(avg_norm_embs)\n",
    "                sleep_len = len(avg_sleep_embs)\n",
    "                all_embs = list(avg_norm_embs) + list(avg_sleep_embs)\n",
    "                pca = PCA(n_components=3)\n",
    "                pri_comps = pca.fit_transform(all_embs)\n",
    "                new_pc_normal_train = pri_comps[:norm_len]\n",
    "                new_pc_sleepy_train = pri_comps[norm_len:]\n",
    "                pc_normal_train = np.concatenate((pc_normal_train, np.array(new_pc_normal_train)), axis=0)\n",
    "                pc_sleepy_train = np.concatenate((pc_sleepy_train, np.array(new_pc_sleepy_train)), axis=0)\n",
    "            else:\n",
    "                pc_normal_train = np.concatenate((pc_normal_train, np.array(avg_norm_embs)), axis=0)\n",
    "                pc_sleepy_train = np.concatenate((pc_sleepy_train, np.array(avg_sleep_embs)), axis=0)\n",
    "                \n",
    "    return pc_normal_train, pc_sleepy_train\n",
    "\n",
    "def multi_subjects_class(subjects, test_subjects):\n",
    "    subjects = list(map(str, subjects))\n",
    "    num_test_subs = int(0.3 * len(subjects))\n",
    "    \n",
    "    if test_subjects:\n",
    "        test_subs = list(map(str, test_subjects))\n",
    "    else:\n",
    "        test_subs = random.sample(subjects, k=num_test_subs)\n",
    "    train_subs = list((Counter(subjects)-Counter(test_subs)).elements())\n",
    "    \n",
    "    print(train_subs)\n",
    "    print(test_subs)\n",
    "    \n",
    "    X_normal_train, X_sleepy_train = obtain_data(train_subs, pca=True)\n",
    "    X_normal_test, X_sleepy_test = obtain_data(test_subs, pca=True)\n",
    "   \n",
    "\n",
    "    y_normal_train = list(np.zeros(len(X_normal_train)))\n",
    "    y_normal_test = list(np.zeros(len(X_normal_test)))\n",
    "\n",
    "    y_sleepy_train = list(np.ones(len(X_sleepy_train)))\n",
    "    y_sleepy_test = list(np.ones(len(X_sleepy_test)))\n",
    "\n",
    "\n",
    "    X_normal = X_normal_train.tolist() + X_normal_test.tolist()\n",
    "    X_sleepy = X_sleepy_train.tolist() + X_sleepy_test.tolist()\n",
    "\n",
    "    X = X_normal + X_sleepy\n",
    "\n",
    "    train_normal_end = len(X_normal_train)\n",
    "    test_normal_end = len(X_normal_test) + train_normal_end\n",
    "\n",
    "    train_sleepy_end = len(X_sleepy_train) + test_normal_end\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_norm = scaler.fit_transform(X)\n",
    "\n",
    "    norm_X_normal_train = X_norm[:train_normal_end]\n",
    "    norm_X_normal_test = X_norm[train_normal_end:test_normal_end]\n",
    "    norm_X_sleepy_train = X_norm[test_normal_end:train_sleepy_end]\n",
    "    norm_X_sleepy_test = X_norm[train_sleepy_end:]\n",
    "\n",
    "    print(len(X_normal_train), len(norm_X_normal_train))\n",
    "    print(len(X_normal_test), len(norm_X_normal_test))\n",
    "    print(len(X_sleepy_train), len(norm_X_sleepy_train))\n",
    "    print(len(X_sleepy_test), len(norm_X_sleepy_test))\n",
    "\n",
    "    y_train = y_normal_train + y_sleepy_train\n",
    "    y_test = y_normal_test + y_sleepy_test\n",
    "\n",
    "    X_train = norm_X_normal_train.tolist() + norm_X_sleepy_train.tolist()\n",
    "    X_test = norm_X_normal_test.tolist() + norm_X_sleepy_test.tolist()\n",
    "\n",
    "    # train model\n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(np.array(X_train), y_train)\n",
    "\n",
    "    y_pred = model.predict(np.array(X_test))\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_round =  round(f1, 2) \n",
    "    print(\"F1 score: \", f1_round)\n",
    "    return model, f1_round, X_train, X_test, y_train, y_test\n",
    "\n",
    "def k_fold_class(subjects, k, test_subjects=None):\n",
    "    sum_f1 = 0\n",
    "    for i in range(k):\n",
    "        model, f1_round, X_train, X_test, y_train, y_test = multi_subjects_class(subjects, test_subjects)\n",
    "        sum_f1 += f1_round\n",
    "    return sum_f1 / k\n",
    "\n",
    "#subjects = [1,3,4,6,7,9,10,11,12,14,15,16,20,22,23,24,25]\n",
    "subjects = [9,10, 11, 12, 14,15,16,20,23,24, 25]\n",
    "test_subjects = [10, 15, 12]\n",
    "\n",
    "#model, f1_round, X_train, X_test, y_train, y_test = multi_subjects_class(subjects)\n",
    "f1 = k_fold_class(subjects, 1, test_subjects)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_plot(comp1, comp2, condition, time, size=40, text = False):\n",
    "    plt.title(\"Deep features of \" + time + \" min segments for subject \" + subject[-2:] + ' in ' + condition + ' condition')\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.scatter(comp1, comp2, label = condition, cmap = 'Blues', c = range(len(comp1)), s=size)\n",
    "    if text:\n",
    "        for i in range(len(comp1)):\n",
    "            plt.annotate(i + 1, (comp1[i], comp2[i]), fontsize = 20)\n",
    "\n",
    "    plt.savefig('../final_figures/deep_gradient_scatter_' + subject + condition + time + '_min.jpg')\n",
    "    plt.show()\n",
    "\n",
    "def average(values, avg_len):\n",
    "    new_values = []\n",
    "    sum_values = []\n",
    "    for i, val in enumerate(values):\n",
    "        sum_values.append(val)\n",
    "        if i % avg_len == 0 and i > 0:\n",
    "            new_values.append(sum(sum_values) / len(sum_values))\n",
    "            sum_values = []\n",
    "            \n",
    "    # if there are rest values\n",
    "    if len(sum_values) > 0:\n",
    "        new_values.append(sum(sum_values) / len(sum_values))\n",
    "    return new_values\n",
    "\n",
    "avg_len = 10\n",
    "avg_comp1_normal = average(pc_normal[:,0], avg_len)\n",
    "avg_comp2_normal = average(pc_normal[:,1], avg_len)\n",
    "\n",
    "gradient_plot(avg_comp1_normal, avg_comp2_normal, 'normal', str(avg_len), size=180, text = True)\n",
    "gradient_plot(pc_normal[:,0], pc_normal[:,1], 'normal', '1')\n",
    "plt.show()\n",
    "\n",
    "avg_comp1_sleepy = average(pc_sleep[:,0], avg_len)\n",
    "avg_comp2_sleepy = average(pc_sleep[:,1], avg_len)\n",
    "\n",
    "gradient_plot(avg_comp1_sleepy, avg_comp2_sleepy, 'sleepy', str(avg_len), size=180, text = True)\n",
    "gradient_plot(pc_sleep[:,0], pc_sleep[:,1], 'sleepy', '1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clustering techniques\n",
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(embs, pri_comps, epsilon, cond):\n",
    "    cluster_model = DBSCAN(eps = epsilon, min_samples = 1).fit(np.array(embs))\n",
    "    labels = cluster_model.labels_\n",
    "    uni_labels = np.unique(labels)\n",
    "    print(labels)\n",
    "\n",
    "    all_values = []\n",
    "    for label in uni_labels:\n",
    "        label_values = []\n",
    "        for i, emb in enumerate(pri_comps):\n",
    "            if labels[i] == label:\n",
    "                label_values.append(list(pri_comps[i]))\n",
    "        plt.scatter(np.array(label_values)[:,0], np.array(label_values)[:,1], label = str(label))\n",
    "\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('DBSCAN Clustering of 1 min segments of subject ' + subject[-2:]  + ' ' + cond + ' condition')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('../final_figures/clusters_' + subject + '_' + cond + '.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "    return labels\n",
    "\n",
    "#all_labels = plot_clusters(all_embs, pri_comps, 0.3, \"all\")\n",
    "normal_labels = plot_clusters(avg_norm_embs, pc_normal, 0.10, \"normal\")\n",
    "sleep_labels = plot_clusters(avg_sleep_embs, pc_sleep, 0.22, \"sleepy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Evolution of clusters trough time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_barplot(labels, cond):\n",
    "    occurences = {}\n",
    "    y_values = []\n",
    "    \n",
    "    all_colors = ['blue', 'orange', 'green', 'red', 'cyan', 'yellow', 'purple', 'olive', 'pink']\n",
    "    colors = []\n",
    "    \n",
    "    for lab in labels:\n",
    "        if lab in occurences:\n",
    "            occurences[lab] += 1\n",
    "        else:\n",
    "            occurences[lab] = 1\n",
    "        y_values.append(occurences[lab])\n",
    "        colors.append(all_colors[lab])\n",
    "    x_values = range(len(labels))\n",
    "    \n",
    "    plt.bar(x_values, y_values, color=colors)\n",
    "    plt.title(\"Evolution of clusters of subject \" + subject[-2:] + \" \" + cond + \" condition using DBSCAN\")\n",
    "    plt.xlabel(\"Time in Minutes\")\n",
    "    plt.ylabel(\"Size of clusters\")\n",
    "    plt.savefig(\"../final_figures/clusters_evolution_\" + subject + \"_\" + cond + \".jpg\")\n",
    "    plt.show()\n",
    "\n",
    "#make_barplot(all_labels, \"all\")\n",
    "make_barplot(normal_labels, \"normal\")\n",
    "make_barplot(sleep_labels, \"sleepy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_data = TSNE(perplexity = 20 ).fit_transform(np.array(all_embs))\n",
    "\n",
    "tsne_normal = tsne_data[:norm_len]\n",
    "tsne_sleep = tsne_data[norm_len:]\n",
    "print(len(tsne_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_normal[:,0], tsne_normal[:,1], label = 'Normal')\n",
    "plt.scatter(tsne_sleep[:,0], tsne_sleep[:,1], label = 'Sleep Restricted')\n",
    "\n",
    "plt.xlabel('T-SNE Component 1')\n",
    "plt.ylabel('T-SNE Component 2')\n",
    "plt.title('Visualizing 1 min segments of subject ' + subject[-2:] + ' using t-SNE')\n",
    "plt.legend()\n",
    "plt.savefig('../final_figures/tsne_plot_subject' + subject[-2:] + '.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = DBSCAN(eps = 0.28, min_samples = 1).fit(np.array(all_embs))\n",
    "labels = cluster_model.labels_\n",
    "uni_labels = np.unique(labels)\n",
    "print(labels)\n",
    "\n",
    "all_values = []\n",
    "for label in uni_labels:\n",
    "    label_values = []\n",
    "    for i, emb in enumerate(tsne_data):\n",
    "        if labels[i] == label:\n",
    "            label_values.append(list(tsne_data[i]))\n",
    "    plt.scatter(np.array(label_values)[:,0], np.array(label_values)[:,1], label = str(label))\n",
    "\n",
    "plt.xlabel('T-sne Component 1')\n",
    "plt.ylabel('T-sne Component 2')\n",
    "plt.title('Clusters of 1 min segments of subject 10')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('../figures/clusters_sub10_tsne.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multiple subjects analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_embs10, sleep_embs10 = load_embeddings('sub10')\n",
    "norm_embs9, sleep_embs9 = load_embeddings('sub9')\n",
    "\n",
    "avg_norm_embs10 = embeddings_segment(norm_embs10, len(norm_embs10), 46 * 60)\n",
    "avg_sleep_embs10 = embeddings_segment(sleep_embs10, len(sleep_embs10), 46 * 60)\n",
    "\n",
    "avg_norm_embs9 = embeddings_segment(norm_embs9, len(norm_embs9), 46 * 60)\n",
    "avg_sleep_embs9 = embeddings_segment(sleep_embs9, len(sleep_embs9), 46 * 60)\n",
    "\n",
    "print(len(norm_embs), len(sleep_embs))\n",
    "\n",
    "all_embs = avg_norm_embs9 + avg_sleep_embs9 + avg_norm_embs10 + avg_sleep_embs10\n",
    "norm_len9 = len(avg_norm_embs9)\n",
    "sleep_len9 = norm_len9 + len(avg_sleep_embs9)\n",
    "norm_len10 = sleep_len9 + len(avg_norm_embs10)\n",
    "sleep_len10 = norm_len10 + len(avg_sleep_embs10)\n",
    "print(len(all_embs))\n",
    "print(sleep_len10)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pri_comps_multi = pca.fit_transform(all_embs)\n",
    "\n",
    "pc_normal9 = pri_comps_multi[:norm_len9]\n",
    "pc_sleep9 = pri_comps_multi[norm_len9: sleep_len9]\n",
    "pc_normal10 = pri_comps_multi[sleep_len9:norm_len10]\n",
    "pc_sleep10 = pri_comps_multi[norm_len10:]\n",
    "\n",
    "print(len(pc_normal9) + len(pc_sleep9) + len(pc_normal10) + len(pc_sleep10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pc_normal9[:,0], pc_normal9[:,1], label = 'Normal Subject 9')\n",
    "plt.scatter(pc_sleep9[:,0], pc_sleep9[:,1], label = 'Sleep Restricted Subject 9')\n",
    "plt.scatter(pc_normal10[:,0], pc_normal10[:,1], label = 'Normal Subject 10')\n",
    "plt.scatter(pc_sleep10[:,0], pc_sleep10[:,1], label = 'Sleep Restricted Subject 10')\n",
    "#plt.scatter(pri_comps[:,0], pri_comps[:,1])\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('The first two principal components of 1 min segments of subject 9 and 10')\n",
    "plt.legend()\n",
    "plt.savefig('pc_plot_subj9&10.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_textfile(embeddings):\n",
    "    with open('../textfiles/' + subject + '/' + subject + '_1', 'w') as f:\n",
    "        for row in embeddings:\n",
    "            first = True\n",
    "            for value in row:\n",
    "                if not first:\n",
    "                    f.write(';')\n",
    "                else:\n",
    "                    first = False\n",
    "                f.write(str(value))\n",
    "            f.write('\\n')\n",
    "    print('done')\n",
    "make_textfile(np.array(avg_norm_embs))\n",
    "print(np.array(avg_norm_embs).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
